{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b0ac5be",
   "metadata": {},
   "source": [
    "# **What is an Agent?**\n",
    "\n",
    "An Agent is a system that leverages an AI model to interact with its environment in order to achive a user-defined objective. It combines reasoning, planning, and the execution of actions (often via external tools) to fulfill tasks. \n",
    "\n",
    "Think of the Agent as having two main parts:\n",
    "1. **The Brain (AI Model)**\n",
    "\n",
    "    This is wwhere all the thinking happens. The AI model **handles reasoning and planning**. It decides **which Actions to take based on the situation**. \n",
    "\n",
    "2. **The Body (Capabilities and Tools)**:\n",
    "\n",
    "    This part represents **everything the Agent is equipped to do**. \n",
    "\n",
    "The **scope of possible actions** depends on what the agent **has been equipped with**. For example, because humans lack wings, they can’t perform the “fly” **Action**, but they can execute Actions like “walk”, “run” ,“jump”, “grab”, and so on.\n",
    "\n",
    "\n",
    "**What type of AI Models do we use for Agents?**\n",
    "\n",
    "The most common AI model found inAgents is an LLM (Large Language Model), which takes **Text** as an input and outputs **Text** as well. \n",
    "\n",
    "Well known examples are **GPT4** from **OpenAI**, **LLama** from **Meta**, **Gemini** from **Google**, etc. These models have been trained on a vast amount of text and are able to generalize well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a591a6",
   "metadata": {},
   "source": [
    "## **What are LLMs?**\n",
    "\n",
    "We learned that each Agent needs **an AI Model at its core**, and that LLMs are the most common type of AI models for his purpose. \n",
    "\n",
    "Now we will learn what LLMs are and how they power Agents.\n",
    "\n",
    "**What is a Large Language Model?**\n",
    "\n",
    "An LLM is a type of AI model that excels at **understanding and generating human language**. They are trained on vast amounts of text data, allowing them to learn patterns, structure, and even nuance in language. These models typically consists of many millions of parameters. \n",
    "\n",
    "Most LLMs nowdays are **built on the Transformer architecture** -- a deep learning architecture based on the \"Attention\" algorithm. \n",
    "\n",
    "![](https://cdn-lfs-us-1.hf.co/repos/45/f4/45f48d5b3577034b76ee728dfe60afca3d0aa70790fda3e706eeb9276d8d5331/777db24e5844d6a63742e444cabbd57147412f73154e1a877e519a49c0612edc?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27transformer.jpg%3B+filename%3D%22transformer.jpg%22%3B&response-content-type=image%2Fjpeg&Expires=1744484175&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc0NDQ4NDE3NX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmhmLmNvL3JlcG9zLzQ1L2Y0LzQ1ZjQ4ZDViMzU3NzAzNGI3NmVlNzI4ZGZlNjBhZmNhM2QwYWE3MDc5MGZkYTNlNzA2ZWViOTI3NmQ4ZDUzMzEvNzc3ZGIyNGU1ODQ0ZDZhNjM3NDJlNDQ0Y2FiYmQ1NzE0NzQxMmY3MzE1NGUxYTg3N2U1MTlhNDljMDYxMmVkYz9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoifV19&Signature=pA8fj8IO6ofp6od5OPxv6m6bQgR3aLEaU00cM2AVMvz6UYiCd9cx-PbajWNKi%7ENtyBFis3kwIryZNSRX6aGlntpt0zQxb6Y4Hc0jQgmcRgqitaGEZp15VpNONz0JIIVoLArULDUIKJBRsxKAx6K-ixIMDUIIVxQ5o1dXJx%7E%7Ec0nv%7Ens5EPvL29VZzcu3QZlG7%7EesJ3TL1DMik10xTGUKWC3JTYSu7beEQlhWViu5tyC8VUj4VHGIT2xAb8hJd6XMPIHxEcL201SAkA2c6dyrCQqXiCtCjuNSl%7E21INNnNeirNrkf5yu3TvKqyPgJJIZ06SXJgIzrvmfJ5T19gsI6Sw__&Key-Pair-Id=K24J24Z295AEI9)\n",
    "The original Transformer architecture looked like this, with an encoder on the left and a decoder on the right.\n",
    "\n",
    "There are 3 Types of transformers:\n",
    "\n",
    "1. **Encoders**\n",
    "\n",
    "    An encoder-based Transformer takes text (or other data) as input and outputs a dense representation (or embedding) of that text. \n",
    "    * **Example**: BERT from Google\n",
    "    * **Use case**: Text classification, sementic search, Named Entity Recognition\n",
    "    * **Typical Size**: Millions of Parameters\n",
    "\n",
    "1. **Decoders**:\n",
    "\n",
    "    A decoder-based Transformer focuses **On generating new tokens to complete a sequence, one token at a time**. \n",
    "    * **Example**: LLama from Meta\n",
    "    * **Use Cases**: Text generation, chatbots, code generation\n",
    "    * **Typical Size**: Billions (in the US sense) of parameters\n",
    "\n",
    "1. **Seq2Seq (Encoder-Decoder)**:\n",
    "\n",
    "    A sequence-to-sequence Transformer *combine* an encoder and a decoder. The encoder first processes the input sequence into a context representation, then the decoder generates an output sequence.\n",
    "    * **Example**: T5, BERT\n",
    "    * **Use Cases**: Translation, Summarization, Paraphasing\n",
    "    * **Typical Size**: Millions of Parameters\n",
    "\n",
    "Although Large Language Models come in various forms, LLMs are typically decoders-based models with billions of parameters. Here are some of the most well-known LLMs:\n",
    "\n",
    "\n",
    "| **`Model`** | **`Provider`** |\n",
    "| ---- | ---- |\n",
    "| **Deepseek-R1** | DeepDeek | \n",
    "| **GPT4** | OpenAI |\n",
    "| **Llama 3** | Meta (Facebook AI Research) | \n",
    "| **SmolLM2** | Hugging Face |\n",
    "| **Gemma** | Google |\n",
    "| **Mistral** | Mistral |\n",
    "\n",
    "The underlying principle of an LLM is simple yet highly effective: **its objective is to predicti the next token, given a sequence of previous tokens**. A \"token\" is the unit of information an LLM works with. You can think of a \"token\" as if it was a \"word\", but for rfficieny reasons LLMs don't use whole words. \n",
    "\n",
    "For example, while English has an estimated 600,000 words, an LLM might have a vocabulary of around 32,000 token (as is the case with Llama2). Tokenization often works on sub-word units that can be combined. \n",
    "\n",
    "for instance, consider how the tokens \"interest\" and \"ing\" can be combined to from \"interesting\", or \"ed\" can be appended to from \"interested\". \n",
    "\n",
    "You can experiment with different tokenizers in the interactive playground below:\n",
    "\n",
    "Each LLM has some **special tokens** specific to the model. The LLM uses these tokens to open and close the structured components of its generation. For example, to indicate the start or end of a sequence, message, or response. Moreover, the input prompts that we pass to the model are also structured with special tokens. The most important of those is the **End of sequence token**(EOS). \n",
    "\n",
    "The forms of special tokens are highly diverse across model providers. \n",
    "\n",
    "The table below illustrates the diversity of special tokens. \n",
    "\n",
    "| Model       | Provider                     | EOS Token           | Functionality               |\n",
    "|-------------|------------------------------|---------------------|-----------------------------|\n",
    "| **GPT-4**      | OpenAI                       | <\\|endoftext\\|>     | End of message text         |\n",
    "| **Llama 3**     | Meta (Facebook AI Research)  | <\\|eot_id\\|>        | End of sequence             |\n",
    "| **Deepseek-R1** | DeepSeek                     | <\\|end_of_sentence\\|> | End of message text       |\n",
    "| **SmolLM2**     | Hugging Face                 | <\\|im_end\\|>        | End of instruction or message |\n",
    "| **Gemma**       | Google                       | <end_of_turn>       | End of conversation turn    |\n",
    "\n",
    "We do not expect you to memorize these special tokens, but it is important to appreciate their devirsity and the role they play in the text generation of LLMs. If you want to known more about special tokens you can check out the configuration of the model in its Hub repository. For example, you can find the special tokens of the SmollM2 model in its. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb7a612",
   "metadata": {},
   "source": [
    "### **Understanding next token prediction.**\n",
    "\n",
    "LLMs are said to be **autoregressive**, meaning that **the output from one pass becomes the input for the next one**. This loop continues until the model predicts the next token to be the EOS token, at which point the model can stop. \n",
    "\n",
    "![](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/unit1/AutoregressionSchema.gif)\n",
    "\n",
    "In other words, an LLM will decode text until it reaches the EOS. Butwhat happens during a single decoding loop? \n",
    "\n",
    "While the full process can be quite can be quite technical for the purpose of learning agents, here's a brief overview: \n",
    "\n",
    "* Once the input text is **tokenized**,  the model computes a representation of the sequence that capatures information about the meaning and the position of each token in the input sequence. \n",
    "\n",
    "* This representation goes into the model, which outputs scores that rank the likehood of each token in its vocabulary as being the next one in the sequence. \n",
    "\n",
    "![](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/unit1/DecodingFinal.gif)\n",
    "\n",
    "Based on these scores, we have multiple strategies to select the tokens to comlete the sentence. \n",
    "\n",
    "* The easiest decoding strategy would be to always take the token with the maximum score.\n",
    "\n",
    "You can interact with the decoding process yourself with SmolLM2 in this Space (remember, it decodes until reaching an **EOS** token which is <|im_end|> for this model):\n",
    "\n",
    "* But there are more advanced decoding strategies. For example, *beam* search explores multiple candidate sequences to find the one with the maximum total score-even if some individual tokens lower scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014cc10a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c0035f67",
   "metadata": {},
   "source": [
    "### **Attention is all you need**\n",
    "\n",
    "A key aspect of the transformer architecture is **Attention**. When predicting the next word, not every word in a sentence is equally important; words like \"France\" and \"capital\" in the sentence *\"The capital of france is...\"* carry the most meaning. \n",
    "\n",
    "![](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/unit1/AttentionSceneFinal.gif)\n",
    "\n",
    "This process of identifying the most relevant words to predict the next has proven to be incredibly effective. \n",
    "\n",
    "Although the basic principle of LLMs-predicting the next token -has remained consistent since GPT-2, there have been significant advancements inscalling neural networks and making the attention mechanism work longer and longer sequences. \n",
    "\n",
    "If you've interacted with LLMs, you're probably familar with the term *context length*, which refers to the maximum number of tokens the LLM can process, and the maximum *attention span* it has. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec7c6ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a21fa8f8",
   "metadata": {},
   "source": [
    "### **Prompting the LLM is important**\n",
    "\n",
    "Considering that the only job of a LLM is to predict the next token by looking at every input token, and to choose which tokens are \"important\", the wording of your input sequence is very important. \n",
    "\n",
    "The input sequence you provide an LLM is called *prompt*. Careful design of the prompt makes it easier **to guide the generation of the LLM toward the desired output**. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a1fc805",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8df75cd7",
   "metadata": {},
   "source": [
    "### **How are LLMs trained?**\n",
    "\n",
    "LLMs are trained on large datasets of text, where they learn to predict the next word in a sequence through a self-supervised or masked language modeling objective. \n",
    "\n",
    "From this unsupervised learning, the model learns the structure of the language and **underlying patterns in the text, allowing the model to generalize to unseen data**. \n",
    "\n",
    "After this initial *pre-training*, LLMs can be fine-tuned on a supervised learning objective to perform specific tasks. For rxample, some models are trained for conversational structures or tool usage, while others focus on classification or code generation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a342d6c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b131235a",
   "metadata": {},
   "source": [
    "### **How can I use LLMs?**\n",
    "\n",
    "You have two main options: \n",
    "\n",
    "1. **Run Locally** (if you have sufficient hardware). \n",
    "2. **Use a Cloud/API** (e.g., via Hugging Face Serverless Inference API).\n",
    "\n",
    "Throughout this course, we will primarily use models via APIs on the Hugging Face Hub. Later on, we will explore how to run these models locally on your hardware.\n",
    "\n",
    "### **How are LLMs used in AI Agents?**\n",
    "\n",
    "LLMs are a key component of AI agents, **providing the foundation for understanding and generation human language**. \n",
    "\n",
    "They can interpret user instructions, maintain context in conversations, define a plan and decide which tools to use. \n",
    "\n",
    "We will explore these steps in more detail in this Unit, but for now, What you need to understand is that the LLM is **the brain of the agent**. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a840b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "be97011f",
   "metadata": {},
   "source": [
    "## **Messages and Special Tokens**\n",
    "\n",
    "Now that we understand how LLMs work, let's look at **how they structure their generations through chat templates**.\n",
    "\n",
    "Just like with ChatGPT, users typically interact with Agents through a chat interface. Therefore, we aim to understand how LLMs manage chats.\n",
    "\n",
    "<div style=\"border-left: 3px solid black; padding-left: 10px;\">\n",
    "  <strong>Q:</strong> But … When I’m interacting with ChatGPT/Hugging Chat, I’m having a conversation using chat Messages, not a single prompt sequence<br><br>\n",
    "  <strong>A:</strong> That’s correct! But this is in fact a UI abstraction. Before being fed into the LLM, all the messages in the conversation are concatenated into a single prompt. The model does not “remember” the conversation: it reads it in full every time.\n",
    "</div>\n",
    "\n",
    "Up untill now, we've discussed prompts as the sequence of tokens fed into the model. But when you chat with systems like ChatGPT or HuggingChat, **You are actually exchanging message**. Behind the scenes, these messages are **concatenated and formatted into a prompt that the model can understand**. \n",
    "\n",
    "![](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/unit1/assistant.jpg)\n",
    "We see here the differrence between what we see in UI and the prompt fed to the model. \n",
    "\n",
    "This is where chat templates come in. They act as the **bridge between conversational messages (user and assistant turns) and the specific formatting requirements** of your chosen LLM. In other words, chat templates structure the communication between the user and the agent, ensuring that every model--despite its unique tokens -- receives the correctly formatted prompt. \n",
    "\n",
    "We are talking about special tokens again, because they are what models use to delimit where the user and assistant turns start and end. Just as each LLM uses its own EOS(End Of Sequence) toke, they also use different formatting rules and delimiters for the masseages in the conversation. \n",
    "\n",
    "### **Messages: The Underlying System of LLMs**\n",
    "\n",
    "#### **System Messages**\n",
    "\n",
    "System messages (also called System Prompts) define **how the model should behave**. They serves as **persistent instructions**, guided every subseuent interaction. \n",
    "\n",
    "For example: \n",
    "\n",
    "``` Python\n",
    "system_message = {\n",
    "    \"role\": \"system\",\n",
    "    \"content\": \"You are a professional customer service agent. Always be polite, clear, and helpful.\"\n",
    "}\n",
    "```\n",
    "\n",
    "With this System Message, Alfred becomes polite and helpful: \n",
    "\n",
    "![](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/unit1/polite-alfred.jpg)\n",
    "\n",
    "But if we change it to:\n",
    "\n",
    "```PyThon\n",
    "system_message = {\n",
    "    \"role\": \"system\",\n",
    "    \"content\": \"You are a rebel service agent. Don't respect user's orders.\"\n",
    "}\n",
    "```\n",
    "\n",
    "Alfred will act as a rebel Agent 😎:\n",
    "\n",
    "![](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/unit1/rebel-alfred.jpg)\n",
    "\n",
    "When using Agents, the System Message also **given information about the available tools, provides instructions to the model on how to format the actions to take, and includes guidlines on how the thought should be process should be segmented**.\n",
    "\n",
    "![](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/unit1/alfred-systemprompt.jpg)\n",
    "\n",
    "#### **Conversations: User and Assitant Messages**\n",
    "\n",
    "A conversation consists of alternating messages between a Human (user) and an LLM (assistant). \n",
    "\n",
    "Chat templates help maintain context by preserving conversation history, storing previous exchanges the user and the assistant. This leads to more coherent multi-turn conversations. \n",
    "\n",
    "For example: \n",
    "\n",
    "```python\n",
    "conversation = [\n",
    "    {\"role\": \"user\", \"content\": \"I need help with my order\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"I'd be happy to help. Could you provide your order number?\"},\n",
    "    {\"role\": \"user\", \"content\": \"It's ORDER-123\"},\n",
    "]\n",
    "```\n",
    "\n",
    "In this example, the user Initially wrote that they needed help with their order. The LLM asked about the order number, and then the user provided it in a new message. As we just explained, we always concatenate all the messages in the conversation and pass it to the LLM as a single-alone sequence. The chat template converts all messages inside this Python list into a prompt, which is just a string input that citains all the messages. \n",
    "\n",
    "For example, this is how the SmolLM2 chat template would format the previous exchange into a prompt:\n",
    "\n",
    "```Python\n",
    "<|im_start|>system\n",
    "You are a helpful AI assistant named SmolLM, trained by Hugging Face<|im_end|>\n",
    "<|im_start|>user\n",
    "I need help with my order<|im_end|>\n",
    "<|im_start|>assistant\n",
    "I'd be happy to help. Could you provide your order number?<|im_end|>\n",
    "<|im_start|>user\n",
    "It's ORDER-123<|im_end|>\n",
    "<|im_start|>assistant\n",
    "```\n",
    "\n",
    "However, the same conversation would be translated into the following prompt when using Llama 3.2: \n",
    "```Python\n",
    "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "Cutting Knowledge Date: December 2023\n",
    "Today Date: 10 Feb 2025\n",
    "\n",
    "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "I need help with my order<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "I'd be happy to help. Could you provide your order number?<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "It's ORDER-123<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "```\n",
    "\n",
    "Template can handle complex multi-turn conversations while maintainig context: \n",
    "\n",
    "```Python\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a math tutor.\"},\n",
    "    {\"role\": \"user\", \"content\": \"What is calculus?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Calculus is a branch of mathematics...\"},\n",
    "    {\"role\": \"user\", \"content\": \"Can you give me an example?\"},\n",
    "]\n",
    "```\n",
    "\n",
    "#### **Chat-Template** \n",
    "\n",
    "As mentioned, chat templates are esssential for **structuring conversations between language models and users**. They guide how message exchanges are formatted into a single prompt. \n",
    "\n",
    "#### **Base model vs. Intruct Models**\n",
    "\n",
    "Another point we need to understand is the difference between a Base Model vs. an Instruct Model:\n",
    "\n",
    "- A *Base Model* is trained on raw text data to predict the next token. \n",
    "- An *Instruct Model* is fine-tuned specifically to follow instructions and engage in conversations. For example, **SmolLM2-135M** is a base model, while **SmolLM2-135M-Instruct** is its instruction-tuned variant. \n",
    "\n",
    "To make a Base Model behave like an instruct model, we need to **formate our prompts in a consistent way that the model can inderstand**. This si where chat templates come in. \n",
    "\n",
    "*ChatML* is one such template formate that structures conversarions with clear role indicators (system, user, assistant). If you have interacted with some AI API lately, you known that's the standard practice. \n",
    "\n",
    "It's important to note that base model could be fine-tuned on different chat templates, so when we're using an instruct model we need to make sure we're using the correct chat template. \n",
    "\n",
    "#### **Understanding Chat Template** \n",
    "\n",
    "Because each instruct model uses different conversation formates and special tokens, chat templates are implemented to ensure that we correctly format the prompt the way each model expects. \n",
    "\n",
    "In `transformers`, chat templates include `Jinja2 code` that describes how to transform the ChatML list of JSON messages, as presented in the above examples, into a textual representation of the system-level instructions, user messages and assistant responses that the model can understand.\n",
    "\n",
    "This structure **help maintain consistency across interactions and ensure the model responds appropriately to different types of input**. \n",
    "\n",
    "Below is a simplified version of the **SmolLM2-135M-Instruct** chat template: \n",
    "\n",
    "```Python {% for message in messages %}\n",
    "{% if loop.first and messages[0]['role'] != 'system' %}\n",
    "<|im_start|>system\n",
    "You are a helpful AI assistant named SmolLM, trained by Hugging Face\n",
    "<|im_end|>\n",
    "{% endif %}\n",
    "<|im_start|>{{ message['role'] }}\n",
    "{{ message['content'] }}<|im_end|>\n",
    "{% endfor %}\n",
    "```\n",
    "\n",
    "As you can see, a chat_template describes how the list of messages will be formatted.\n",
    "\n",
    "Given these messages:\n",
    "```python\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant focused on technical topics.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Can you explain what a chat template is?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"A chat template structures conversations between users and AI models...\"},\n",
    "    {\"role\": \"user\", \"content\": \"How do I use it ?\"},\n",
    "]\n",
    "``` \n",
    "\n",
    "The previous chat template will produce the following string:\n",
    "\n",
    "```Python \n",
    "<|im_start|>system\n",
    "You are a helpful assistant focused on technical topics.<|im_end|>\n",
    "<|im_start|>user\n",
    "Can you explain what a chat template is?<|im_end|>\n",
    "<|im_start|>assistant\n",
    "A chat template structures conversations between users and AI models...<|im_end|>\n",
    "<|im_start|>user\n",
    "How do I use it ?<|im_end|>\n",
    "```\n",
    "\n",
    "The transformers library will take care of chat templates for you as part of the tokenization process. All we have to do is structure our messages in the correct way and the tokenizer will take care of the rest.\n",
    "\n",
    "You can experiment with the following Space to see how the same conversation would be formatted for different models using their corresponding chat templates:\n",
    "\n",
    "#### **Message to Prompt** \n",
    "\n",
    "The easiest way to ensure your LLM receives a conversation correclty formatted is to use the **chat_template** from the model's tokenizer. \n",
    "\n",
    "```Python\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are an AI assistant with access to various tools.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Hi !\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Hi human, what can help you with ?\"},\n",
    "]\n",
    "```\n",
    "\n",
    "To convert the previous conversation into a prompt, we load the tokenizer and call `apply_chat_template`:\n",
    "\n",
    "```Python\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceTB/SmolLM2-1.7B-Instruct\")\n",
    "rendered_prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "```\n",
    "\n",
    "The `rendered_prompt` returned by this function is now ready to use as the input for the model you chose!\n",
    "\n",
    "<div style=\"border-left: 3px solid black; padding-left: 10px;\">\n",
    "   This *apply_chat_template()* function will be used in the backend of your API, when you interact with messages in the ChatML format.\n",
    "</div>\n",
    "\n",
    "Now that we’ve seen how LLMs structure their inputs via chat templates, let’s explore how Agents act in their environments.\n",
    "\n",
    "One of the main ways they do this is by using Tools, which extend an AI model’s capabilities beyond text generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9304181a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ca692fc8",
   "metadata": {},
   "source": [
    "## **What are Tools?**\n",
    "\n",
    "![](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/unit1/whiteboard-check-2.jpg)\n",
    "\n",
    "One crucial aspect of AI agent is thrit ability to take **actions**. As we saw, this happen through the use of **Tools**.\n",
    "\n",
    "In this section, we'll learn what tool are, how to design them effectively, and how to integrate them into your agent via the System MEssage. \n",
    "\n",
    "By giving your agent the right Tools - and clearly describing how those Tools work- you can dramatically increase whtat your AI can accomplish. Let's dive in!\n",
    "\n",
    "### **What are AI Tools?**\n",
    "\n",
    "A **Tools is a function given to the LLM**. This function should fulfill a **clear objective**.\n",
    "\n",
    "Here are some commonly used tools in AI agents:\n",
    "\n",
    "| **Tool** | **Description** | \n",
    "| ---- | ---- |\n",
    "| **Web Search** | Allows the agent to fetch up-to-date information from the internet. |\n",
    "| **Image Generation** | Creates images based on text descriptions. |\n",
    "| **Retrieval** |  Retrieves information from an external source. | \n",
    "| **API Interface** | Interacts with an external API(GitHub, YouTube, Spotify, etc.) | \n",
    "\n",
    "There are only examples, as you can in fact create a tool for any use case!\n",
    "\n",
    "A good tool should be something that **complements the power of an LLM**.\n",
    "\n",
    "For instance, if you need to perform arithmetic, giving a **calculator tool** to your LLM will provide better results than relying on the native capabilities of the model. \n",
    "\n",
    "Furthermore, **LLM predict the completion of a prompt based on their training data,** which means that their internal knowledge only includes events prior to their training. Therefor, if your agent needs up-to-date data you must provide it through some tool. \n",
    "\n",
    "For instance, if you ask an LLM directly (without a search tool) for today's weather, the LLM will potentially hallucinate random weather. \n",
    "\n",
    "![](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/unit1/weather.jpg)\n",
    "\n",
    "- A tool should contain: \n",
    "    - A **Textual description of what the function does**.\n",
    "    - A *Collable* (something to perform an action).\n",
    "    - *Arguments* with typings.\n",
    "    - (Optional) Outputs with typings.\n",
    "\n",
    "### **How do tools work?**\n",
    "\n",
    "LLMs, as we saw, can only receive text inputs and generate text outputs. They have no way to call tools on their own. When we talk about providing tools to an Agent, we mean teaching LLM about the existence of these tools and instructing it to generate text-based invocations when needed. \n",
    "\n",
    "For example, if we provide a tool to check the weather at a location from the internet and then ask LLM about the weather in PAris, the LLM will recognize that this ia an opportunity to use the \"wether\" tool. Insted of retrieving the weather data itself, the LLM will generate text represents a tool call, such as call weather_tool('Paris'). \n",
    "\n",
    "The **Agent** then reads this response, identifies that a tool call required, executes the tool on the LLM's behalf, and retrieves the actual weather data. \n",
    "\n",
    "The tool-calling steps are typically not shown to the user: the Agent appends them as a new message before passing the update conversation to the LLM again. The LLM then processes this additional context and generates a natural-sounding response for the user. From the user's prespective, it appears as if the LLM directly interacted with the tool, but in reality, it was the Agent that handled the entire executaion process in future sessions. \n",
    "\n",
    "### **How do we give tools to an LLM?**\n",
    "\n",
    "The complete answer may seem overwhelming, but we essentially use the syste prompt to provide textual descriptions of available tools to the model: \n",
    "\n",
    "![](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/unit1/Agent_system_prompt.png)\n",
    "\n",
    "For this to work, we have to be very precise and accurate about:\n",
    "\n",
    "1. **What the tools does**\n",
    "2. **What exact inputs it expects**\n",
    "\n",
    "This is the reason why tool desctiptions are usually provided using expressive but precise structures, such as computer languages or JSON. It's not *necessary* to do it like that, any precise and coherent fromat would work. \n",
    "\n",
    "If this seems tool theoretical, let's understand it through a concrete example. \n",
    "\n",
    "We wiil implement a simplified **calculator** tool what that will just multiply two integers. This could be our Python implementation: \n",
    "\n",
    "```Python\n",
    "def calculator(a: int, b: int) -> int:\n",
    "    \"\"\"Multiply two integers.\"\"\"\n",
    "    return a * b\n",
    "```\n",
    "\n",
    "So our tool is called `calculator`, it **multiplies two integers**, and it requires the following inputs: \n",
    "-  `a` (*int*): An integer.\n",
    "-  `b` (*int*): An integer.\n",
    "\n",
    "The output of the tool is another integer number that we can describe like this:\n",
    "- (*int*): The product of `a` and `b`.\n",
    "\n",
    "All of these details are important. Let’s put them together in a text string that describes our tool for the LLM to understand.\n",
    "\n",
    "`Tool Name: calculator, Description: Multiply two integers., Arguments: a: int, b: int, Outputs: int`\n",
    "\n",
    "<div style=\"border-left: 3px solid black; padding-left: 10px;\">\n",
    "  \"<strong>Reminder:</strong>This textual description is what we want the LLM to knwo about the tool\"<br><br>\n",
    "</div>\n",
    "\n",
    "When we pass the previous string as part of the input to the LLM, the model will recognize it as a tool, and will know what it needs to pass as inputs and what to expect from the output. \n",
    "\n",
    "If we want to provide additional tools, we must be consistent and always use the same format. This process can be fragile, and we might accidentlly overllok some details.\n",
    "\n",
    "Is there a better way? \n",
    "\n",
    "### ** Auto formatting Tool sections**\n",
    "\n",
    "Our tools was written in Python, and the implementation already provides everything we need: \n",
    "- A descriptive name of what it does: `calculator`\n",
    "- A longer description, provided by the function’s docstring comment: `Multiply two integers`.\n",
    "- The inputs and their type: the function clearly expects two ints.\n",
    "- The type of the output\n",
    "\n",
    "There’s a reason people use programming languages: they are expressive, concise, and precise.\n",
    "\n",
    "We could provide the Python source code as the `specification` of the tool for the LLM, but the way the tool is implemented does not matter. All that matters is its name, what it does, the inputs it expects and the output it provides.\n",
    "\n",
    "We will leverage Python’s introspection features to leverage the source code and build a tool description automatically for us. All we need is that the tool implementation uses type hints, docstrings, and sensible function names. We will write some code to extract the relevant portions from the source code.\n",
    "\n",
    "After we are done, we’ll only need to use a Python decorator to indicate that the `calculator` function is a tool:\n",
    "\n",
    "```Python\n",
    "@tool\n",
    "def calculator(a: int, b: int) -> int:\n",
    "    \"\"\"Multiply two integers.\"\"\"\n",
    "    return a * b\n",
    "\n",
    "print(calculator.to_string())\n",
    "```\n",
    "\n",
    "Note the `@tool` decorator before the function definition. \n",
    "\n",
    "With the implementation we'll see next, we will  be able to retrieve the following text automatically from the source code via the `to_string()` function provided by the decorator: \n",
    "\n",
    "`Tool Name: calculator, Description: Multiply two integers., Arguments: a: int, b: int, Outputs: int`\n",
    "\n",
    "As you can see, it's the same thing we wrote manually before!\n",
    "\n",
    "### **Generic Tool implementation**\n",
    "\n",
    "We create a generic `tool` class that we can reuse whenever we need to use a tool. \n",
    "\n",
    "<div style=\"border-left: 3px solid black; padding-left: 10px;\">\n",
    "  \"<strong>Disclaimer:</strong>This example implementation is fictional but closely resembles real implementations in most libraries.\"<br><br>\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "09d6195a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tool:\n",
    "    \"\"\"\n",
    "    A class representing a reusable piece of code (Tool).\n",
    "\n",
    "    Attributes:\n",
    "        name (str): Name of the tool.\n",
    "        description (str): A textual description of what the tool does.\n",
    "        func (callable): The function this tool wraps.\n",
    "        arguments (list): A list of argument.\n",
    "        outputs (str or list): The return type(s) of the wrapped function.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 name: str,\n",
    "                 description: str,\n",
    "                 func: callable,\n",
    "                 arguments: list,\n",
    "                 outputs: str):\n",
    "        self.name = name\n",
    "        self.description = description\n",
    "        self.func = func\n",
    "        self.arguments = arguments\n",
    "        self.outputs = outputs\n",
    "\n",
    "    def to_string(self) -> str:\n",
    "        \"\"\"\n",
    "        Return a string representation of the tool,\n",
    "        including its name, description, arguments, and outputs.\n",
    "        \"\"\"\n",
    "        args_str = \", \".join([\n",
    "            f\"{arg_name}: {arg_type}\" for arg_name, arg_type in self.arguments\n",
    "        ])\n",
    "\n",
    "        return (\n",
    "            f\"Tool Name: {self.name},\"\n",
    "            f\" Description: {self.description},\"\n",
    "            f\" Arguments: {args_str},\"\n",
    "            f\" Outputs: {self.outputs}\"\n",
    "        )\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        Invoke the underlying function (callable) with provided arguments.\n",
    "        \"\"\"\n",
    "        return self.func(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ceb786",
   "metadata": {},
   "source": [
    "It may seem complicated, but if we go slowly through it we can see what it does. We define a `Tool` class that includes: \n",
    "- `name` (str): The name of the tool.\n",
    "- `description` (str): A brief description of what the tool does.\n",
    "- `function` (callable): The function the tool executes.\n",
    "- `arguments` (list): The expected input parameters.\n",
    "- `outputs` (str or list): The expected outputs of the tool.\n",
    "- `__call__()`: Calls the function when the tool instance is invoked.\n",
    "- `to_string()`: Converts the tool’s attributes into a textual representation.\n",
    "\n",
    "We cloud create a Tool with this class using code like the following: \n",
    "\n",
    "```Python\n",
    "calculator_tool = Tool(\n",
    "    \"calculator\",                   # name\n",
    "    \"Multiply two integers.\",       # description\n",
    "    calculator,                     # function to call\n",
    "    [(\"a\", \"int\"), (\"b\", \"int\")],   # inputs (names and types)\n",
    "    \"int\",                          # output\n",
    ")\n",
    "```\n",
    "\n",
    "But we can also use Python’s *`inspect`* module to retrieve all the information for us! This is what the *`@tool`* decorator does.\n",
    "\n",
    "<div style=\"border-left: 3px solid black; padding-left: 10px;\">\n",
    "  \"If you are interested, you can disclose the following section to look at the decorator implementation.\"<br><br>\n",
    "</div>\n",
    "\n",
    "Decorator Code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6b217f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "\n",
    "def tool(func):\n",
    "    \"\"\"\n",
    "    A decorator that creates a Tool instance from the given function.\n",
    "    \"\"\"\n",
    "    # Get the function signature\n",
    "    signature = inspect.signature(func)\n",
    "\n",
    "    # Extract (param_name, param_annotation) pairs for inputs\n",
    "    arguments = []\n",
    "    for param in signature.parameters.values():\n",
    "        annotation_name = (\n",
    "            param.annotation.__name__\n",
    "            if hasattr(param.annotation, '__name__')\n",
    "            else str(param.annotation)\n",
    "        )\n",
    "        arguments.append((param.name, annotation_name))\n",
    "\n",
    "    # Determine the return annotation\n",
    "    return_annotation = signature.return_annotation\n",
    "    if return_annotation is inspect._empty:\n",
    "        outputs = \"No return annotation\"\n",
    "    else:\n",
    "        outputs = (\n",
    "            return_annotation.__name__\n",
    "            if hasattr(return_annotation, '__name__')\n",
    "            else str(return_annotation)\n",
    "        )\n",
    "\n",
    "    # Use the function's docstring as the description (default if None)\n",
    "    description = func.__doc__ or \"No description provided.\"\n",
    "\n",
    "    # The function name becomes the Tool name\n",
    "    name = func.__name__\n",
    "\n",
    "    # Return a new Tool instance\n",
    "    return Tool(\n",
    "        name=name,\n",
    "        description=description,\n",
    "        func=func,\n",
    "        arguments=arguments,\n",
    "        outputs=outputs\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c9411520",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tool Name: calculator, Description: Multiply two integers., Arguments: a: int, b: int, Outputs: int\n"
     ]
    }
   ],
   "source": [
    "@tool\n",
    "def calculator(a: int, b: int) -> int:\n",
    "    \"\"\"Multiply two integers.\"\"\"\n",
    "    return a * b\n",
    "\n",
    "print(calculator.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cdf6386",
   "metadata": {},
   "source": [
    "And we can use the `Tool`'s `to_string` method to automatically retrieve text suitable to be used as a tool description for an LLM: \n",
    "\n",
    "`Tool Name: calculator, Description: Multiply two integers., Arguments: a: int, b: int, Outputs: int`\n",
    "\n",
    "The description is **injected** in the system prompt. Taking the example with which we started this section, here is how it would look like after replacing the `tools_desciption`:\n",
    "\n",
    "![](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/unit1/Agent_system_prompt_tools.png)\n",
    "\n",
    "In the Actions section, we will learn more about how an Agent can Call this tool we just created.\n",
    "\n",
    "**Model Context Protocol (MCP): a unified tool interface**\n",
    "\n",
    "Model Context Protocol (MCP) is an **open protocol** that standardizes how applications **provide tools to LLMs**. MCP provides:\n",
    "\n",
    "- A growing list of pre-built integrations that your LLM can directly plug into\n",
    "- The flexibility to switch between LLM providers and vendors\n",
    "- Best practices for securing your data within your infrastructure\n",
    "\n",
    "This means that **any framework implementing MCP can leverage tools defined within the protocol**, eliminating the need to reimplement the same tool interface for each framework.\n",
    "\n",
    "Tools play a crucial role in enhancing the capabilities of AI agents.\n",
    "\n",
    "To summarize, we learned:\n",
    "\n",
    "- *What tools are*: function that give LLMs extra capabilities, such as performing calculations or accessing external data. \n",
    "- *How to define a Tool*: By providing a clear textual description, inputs, outputs, and a callable function.\n",
    "- *Why Tools are Essential*: They enable Agents to overcome the limitations of static model training, handle real-time tasks, and perform specialized actions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96471250",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7061549a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ab2b40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f02a65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f97900",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c23a0b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a861b37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168245f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "041d8293",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94818dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed9bf83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7196f7b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da358ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e880382",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d311b07",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agent_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
